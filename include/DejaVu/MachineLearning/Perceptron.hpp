#pragma once

#include <DejaVu/types.hpp>

namespace djv
{
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
	/// \addtogroup MachineLearning
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////

	namespace perceptrons
	{
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \addtogroup perceptrons
		/// \ingroup MachineLearning
		/// \{
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Abstract cass, base model for a perceptron.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class PerceptronBase
		{
			public:

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Common constructor for every perceptron.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				PerceptronBase(uint64_t inputSize);

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the perceptron to a feature.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				float operator()(const scp::Vec<float>& x) const;

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the perceptron to a feature but save every intermediary step to avoid recomputation.
				/// 
				/// For an input feature `x`, this function compute the activation `a` and the dot product of `x` with
				/// the weights plus the bias in `z`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				void goThrough(const scp::Vec<float>& x, float& a, float& z) const;
				void train(const scp::Vec<float>& x, float y, float learningRate = 0.005f);
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Computes the correction to be applied and the error for back propagation.
				/// 
				/// For an input feature `x`, a computed output error `err`, a computed activation `a`, a computed
				/// intermediary result `z` and a learning rate `learningRate`, this function compute the back
				/// propagation error term and a vector of corrections of size `inputSize + 1`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				void computeCorrection(const scp::Vec<float>& x, float err, float a, float z, float learningRate, float& nextErr, scp::Vec<float>& correction) const;
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the correction computed by computeCorrection.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				void applyCorrection(const scp::Vec<float>& correction);

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Returns the activation function evaluated in `z`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual float f(float z) const = 0;
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Returns the derivative of the activation function evaluated in `z`.
				/// 
				/// Most commons activation functions have derivative that can be expressed easily using the image of
				/// the function. That is why in addition to the value the derivative of the activation function must
				/// be computed (`z`), the result of the activation function in `z` is also given: `a`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual float df(float a, float z) const = 0;

				const scp::Vec<float>& getWeights() const;        ///< Returns the weights of the perceptron.
				void setWeights(const scp::Vec<float>& weights);  ///< Sets the weights of the perceptron.
				float getBias() const;                            ///< Returns the bias of the perceptron.
				void setBias(float bias);                         ///< Sets the bias of the perceptron.

			protected:

				scp::Vec<float> _weights;
				float _bias;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Binary Step activation function.
		/// 
		/// The activation is: \f$a(z) = 0\f$ if \f$z < 0\f$ else \f$1\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class BinaryStep : public PerceptronBase
		{
			public:

				BinaryStep(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Leaky Rectified Linear Unit activation function.
		/// 
		/// The activation is: \f$a(z) = -0.1z\f$ if \f$z < 0\f$ else \f$z\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class LeakyReLU : public PerceptronBase
		{
			public:

				LeakyReLU(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Linear activation function.
		/// 
		/// The activation is: \f$a(z) = z\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class Linear : public PerceptronBase
		{
			public:

				Linear(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Rectified Linear Unit activation function.
		/// 
		/// The activation is: \f$a(z) = 0\f$ if \f$z < 0\f$ else \f$z\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class ReLU : public PerceptronBase
		{
			public:

				ReLU(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Sigmoid activation function.
		/// 
		/// The activation is: \f$a(z) = \frac{1}{1 + e^{-z}}\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class Sigmoid: public PerceptronBase
		{
			public:

				Sigmoid(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Perceptron with Sigmoid activation function.
		/// 
		/// The activation is: \f$a(z) = tanh(z)\f$.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class Tanh : public PerceptronBase
		{
			public:

				Tanh(uint64_t inputSize);

				float f(float z) const;
				float df(float a, float z) const;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \}
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
	}
}
