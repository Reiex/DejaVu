#pragma once

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
/// \file
/// \brief Functions and classes for neural networks creation and manipulation.
/// \author Reiex
/// 
/// For a more detailed description, see class NeuralNetwork.
/// 
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

#include <DejaVu/types.hpp>

namespace djv
{
	namespace layers
	{
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \addtogroup layers
		/// \ingroup MachineLearning
		/// \{
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Abstract class, base model for a NeuralNetwork layer.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class LayerBase
		{
			public:

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Common constructor to every layer
				/// 
				/// As all kind of layers do not have separated internal units (like perceptrons), we have chosen to
				/// characterize a layer by its input and output vector sizes.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				LayerBase(uint64_t inputSize, uint64_t outputSize);

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the layer to a feature.
				/// 
				/// The returned vector must be of size `outputSize`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual scp::Vec<float> operator()(const scp::Vec<float>& x) const = 0;

				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the layer to a feature but save every intermediary step to avoid recomputation.
				/// 
				/// For an input feature `x`, this function compute the output `a` and an intermediary vector of size
				/// `outputSize`: `z`.
				/// For instance, for a perceptrons layer, `z` correspond to the dot product of weights and `x` for
				/// each perceptron (plus the biases) and `a` to the activation function applied to each element of
				/// `z`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual void goThrough(const scp::Vec<float>& x, scp::Vec<float>& a, scp::Vec<float>& z) const = 0;
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Computes the correction to be applied and the error of the layer for back propagation.
				/// 
				/// For an input feature `x`, a computed output error `err`, a computed output `a`, a computed
				/// intermediary result `z` and a learning rate `learningRate`, this function compute the output error
				/// vector for the precedent layer and a matrix of corrections of size `outputSize*(inputSize + 1)`.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual void computeCorrection(const scp::Vec<float>& x, const scp::Vec<float>& err, const scp::Vec<float>& a, const scp::Vec<float>& z, float learningRate, scp::Vec<float>& nextErr, scp::Mat<float>& correction) const = 0;
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				/// \brief Applies the correction computed by computeCorrection.
				///////////////////////////////////////////////////////////////////////////////////////////////////////
				virtual void applyCorrection(const scp::Mat<float>& correction) = 0;

				virtual uint64_t getInputSize() const;   ///< Returns the layer's input size.
				virtual uint64_t getOutputSize() const;  ///< Returns the layer's output size.

			protected:

				uint64_t _inputSize;
				uint64_t _outputSize;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Template class representing a neural network's layer of perceptrons.
		/// 
		/// Classes that can be used as TPerceptron are classes derived from djv::perceptrons::PerceptronBase
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		template<typename TPerceptron = perceptrons::Sigmoid>
		class Perceptrons : public LayerBase
		{
			public:

				Perceptrons(uint64_t inputSize, uint64_t outputSize);

				scp::Vec<float> operator()(const scp::Vec<float>& x) const;

				void goThrough(const scp::Vec<float>& x, scp::Vec<float>& a, scp::Vec<float>& z) const;
				void computeCorrection(const scp::Vec<float>& x, const scp::Vec<float>& err, const scp::Vec<float>& a, const scp::Vec<float>& z, float learningRate, scp::Vec<float>& nextErr, scp::Mat<float>& correction) const;
				void applyCorrection(const scp::Mat<float>& correction);

			private:

				std::vector<TPerceptron> _neurons;
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \brief Class representing a neural network's softmax layer.
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		class SoftMax : public LayerBase
		{
			public:

				SoftMax(uint64_t inputSize, uint64_t outputSize);

				scp::Vec<float> operator()(const scp::Vec<float>& x) const;

				void goThrough(const scp::Vec<float>& x, scp::Vec<float>& a, scp::Vec<float>& z) const;
				void computeCorrection(const scp::Vec<float>& x, const scp::Vec<float>& err, const scp::Vec<float>& a, const scp::Vec<float>& z, float learningRate, scp::Vec<float>& nextErr, scp::Mat<float>& correction) const;
				void applyCorrection(const scp::Mat<float>& correction);
		};

		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
		/// \}
		///////////////////////////////////////////////////////////////////////////////////////////////////////////////
	}

	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
	/// \addtogroup MachineLearning
	/// \{
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////

	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
	/// \brief Class representing a full neural network.
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
	class NeuralNetwork
	{
		public:

			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			/// \brief Sets an input layer to the neural network.
			/// 
			/// This method has to be called once for the first layer. Then NeuralNetwork::appendLayer must be used.
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			template<typename TLayer = layers::Perceptrons<perceptrons::Sigmoid>>
			void setInputLayer(uint64_t inputSize, uint64_t outputSize);
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			/// \brief Appends a layer to the neural network.
			/// 
			/// The layer's input size is fixed as the precedent layer's output size. This is why to be called, a first
			/// input layer must have been set using NeuralNetwork::setInputLayer.
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			template<typename TLayer = layers::Perceptrons<perceptrons::Sigmoid>>
			void appendLayer(uint64_t outputSize);

			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			/// \brief Compute the output of the neural network for an input feature vector `x`.
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			scp::Vec<float> operator()(const scp::Vec<float>& x) const;

			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			/// \brief Trains the neural network on a single training feature, the expected output and a learning rate.
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			void train(const scp::Vec<float>& x, const scp::Vec<float>& y, float learningRate = 0.01f);
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			/// \brief Trains the neural network on a batch of training features and expected outputs.
			///////////////////////////////////////////////////////////////////////////////////////////////////////////
			void batchTrain(const std::vector<scp::Vec<float>>& x, const std::vector<scp::Vec<float>>& y, float learningRate = 0.01f);

		private:

			std::vector<std::unique_ptr<layers::LayerBase>> _layers;
	};

	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
	/// \}
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
}

#include <DejaVu/MachineLearning/NeuralNetworkT.hpp>
